{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fk/tl38z_650pnb8d8bzsdfs50h0000gn/T/ipykernel_32095/1830354490.py:1: DtypeWarning: Columns (40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train = pd.read_csv('data/test.csv')\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16737"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to create a record of the original list of ID before we do data processing\n",
    "# this is to ensure we do not loose the original set of IDs that we will require for submission to kaggle\n",
    "test_original_id = train[['id']].copy()\n",
    "test_original_id.head()\n",
    "len(test_original_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing all heading to lowercase\n",
    "train = train.rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling in missing postal codes for two addresses in the data set\n",
    "train.loc[train['address'] == '215, CHOA CHU KANG CTRL', 'postal'] = '680215'\n",
    "train.loc[train['address'] == '238, COMPASSVALE WALK', 'postal'] = '540238'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column for hdb_age_when_sold\n",
    "# this is to show the remaining years left on the lease as of the year transacted\n",
    "hdb_age_when_sold = train[\"tranc_year\"] - train[\"lease_commence_date\"]\n",
    "train.insert(loc = 9, column = \"hdb_age_when_sold\", value = hdb_age_when_sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all other columns for age of the flat that will no longer be used\n",
    "train.drop(columns=['tranc_yearmonth', 'lease_commence_date', 'year_completed', 'hdb_age'],\n",
    "       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the all except town for location pinpointing\n",
    "train.drop(columns=['block', 'street_name', 'address', 'planning_area','postal'], \n",
    "       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns with latitudes and longitudes\n",
    "train.drop(columns=['latitude', 'longitude', 'mrt_latitude', 'mrt_longitude', 'bus_stop_latitude',\n",
    "       'bus_stop_longitude', 'pri_sch_latitude', 'pri_sch_longitude', 'sec_sch_latitude', 'sec_sch_longitude'], \n",
    "       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep mid_storey to represent the location of the flat in the block of flats\n",
    "train.drop(columns=['storey_range','lower','upper','mid'], \n",
    "       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop full flat type and sqm\n",
    "train.drop(columns=['floor_area_sqm', 'full_flat_type'], \n",
    "       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute new boolean column for whether flat has rental units\n",
    "train['has_rental'] = (train[['1room_rental', '2room_rental', '3room_rental', 'other_room_rental']].sum(axis=1) > 0).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all sold and all rental columns (except the new has_rental column)\n",
    "train.drop(columns=['1room_sold','2room_sold','3room_sold','4room_sold','5room_sold','exec_sold','multigen_sold','studio_apartment_sold', '1room_rental', '2room_rental', '3room_rental', 'other_room_rental'], \n",
    "       inplace=True)\n",
    "train.insert(16, 'has_rental', train.pop('has_rental'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop residential\n",
    "train.drop(columns=['residential'], \n",
    "       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns for hawker and malls\n",
    "train.drop(columns=['mall_within_500m', 'mall_within_1km', 'hawker_within_500m', 'hawker_within_1km', 'market_hawker'],\n",
    "       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping all other irrelevant columns\n",
    "train.drop(columns=['mrt_name', 'bus_stop_name', 'pri_sch_name', 'sec_sch_name'],\n",
    "       inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling in missing values \n",
    "train['mall_within_2km'] = train['mall_within_2km'].fillna(value=0)\n",
    "train['hawker_within_2km'] = train['hawker_within_2km'].fillna(value=0)\n",
    "train.dropna(subset = ['mall_nearest_distance'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing type from int to object\n",
    "train['tranc_month'] = train['tranc_month'].astype(object)\n",
    "\n",
    "#changing type from object to bool\n",
    "train['commercial'] = train['commercial'].map({'Y': 1, 'N': 0})\n",
    "train['multistorey_carpark'] = train['multistorey_carpark'].map({'Y': 1, 'N': 0})\n",
    "train['precinct_pavilion'] = train['precinct_pavilion'].map({'Y': 1, 'N': 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 31)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering all terrace houses from the data set\n",
    "filtered_all_terrace = train[train['flat_model'] == 'Terrace']\n",
    "filtered_all_terrace.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16643, 31)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping all terrace houses from the data set\n",
    "train = train.drop(filtered_all_terrace.index)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the town to the region and tagging whether it is mature or non-mature estate\n",
    "town_region_mapping = {'SEMBAWANG': 'NORTH',\n",
    "                  'WOODLANDS': 'NORTH',\n",
    "                  'YISHUN': 'NORTH',\n",
    "                  'ANG MO KIO': 'NORTH-EAST MATURE',\n",
    "                  'HOUGANG': 'NORTH-EAST',\n",
    "                  'PUNGGOL': 'NORTH-EAST',\n",
    "                  'SENGKANG': 'NORTH-EAST',\n",
    "                  'SERANGOON': 'NORTH-EAST MATURE',\n",
    "                  'BEDOK': 'EAST MATURE',\n",
    "                  'PASIR RIS': 'EAST MATURE',\n",
    "                  'TAMPINES': 'EAST MATURE',\n",
    "                  'BUKIT BATOK': 'WEST',\n",
    "                  'BUKIT PANJANG': 'WEST',\n",
    "                  'CHOA CHU KANG': 'WEST',\n",
    "                  'CLEMENTI': 'WEST MATURE',\n",
    "                  'JURONG EAST': 'WEST',\n",
    "                  'JURONG WEST': 'WEST',\n",
    "                  'BISHAN': 'CENTRAL MATURE',\n",
    "                  'BUKIT MERAH': 'CENTRAL MATURE',\n",
    "                  'BUKIT TIMAH': 'CENTRAL MATURE',\n",
    "                  'CENTRAL AREA': 'CENTRAL MATURE',\n",
    "                  'GEYLANG': 'CENTRAL MATURE',\n",
    "                  'KALLANG/WHAMPOA': 'CENTRAL MATURE',\n",
    "                  'MARINE PARADE': 'CENTRAL MATURE',\n",
    "                  'QUEENSTOWN': 'CENTRAL MATURE',\n",
    "                  'TOA PAYOH': 'CENTRAL MATURE'\n",
    "                }\n",
    "train['region_maturity'] = np.vectorize(town_region_mapping.get)(train['town'])\n",
    "\n",
    "#reorganising the new column in sequence for clarity\n",
    "train.insert(0, 'region_maturity', train.pop('region_maturity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the different categories \n",
    "standard_models = ['Standard', 'New Generation', 'Simplified', 'Model A', 'Model A2', 'Improved', 'Apartment', 'Premium Apartment']\n",
    "maisonette_models = ['Maisonette', 'Improved-Maisonette', 'Model A-Maisonette', 'Premium Maisonette', 'Premium Apartment Loft']\n",
    "private_design_models = ['DBSS', 'Type S1', 'Type S2']\n",
    "\n",
    "#assigning the different rows in the df to their respective categories\n",
    "conditions = [\n",
    "    train['flat_model'].isin(standard_models),\n",
    "    train['flat_model'].isin(maisonette_models),\n",
    "    train['flat_model'].isin(private_design_models)\n",
    "]\n",
    "\n",
    "#creating a new column called model_category containing the newly defined categories\n",
    "values = ['Standard', 'Maisonette', 'Private Design']\n",
    "train['model_category'] = np.select(conditions, values, default='Unknown')\n",
    "\n",
    "#reorganising the new column in sequence for clarity\n",
    "train.insert(4, 'model_category', train.pop('model_category'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping rows with the unused flat model (adjoined and 2-room)\n",
    "filtered_adj_2r = train[(train['flat_model'] == 'Adjoined flat') | (train['flat_model'] == '2-room')]\n",
    "train = train.drop(filtered_adj_2r.index)\n",
    "\n",
    "#dropping rows with the unused flat types (1 room and multi-gen)\n",
    "filtered_multigen_1r = train[(train['flat_type'] == '1 ROOM') | (train['flat_type'] == 'MULTI-GENERATION')]\n",
    "train = train.drop(filtered_multigen_1r.index)\n",
    "\n",
    "#dropping the unused columns flat_model, town \n",
    "train.drop(columns = ['flat_model','town'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating column units_per_floor\n",
    "train['units_per_floor'] = train['total_dwelling_units'] / train['max_floor_lvl']\n",
    "\n",
    "#insert column units_per_floor at index 3\n",
    "train.insert(3, 'units_per_floor', train.pop('units_per_floor'))\n",
    "\n",
    "#dropping the previously engineered columns\n",
    "train.drop(columns = ['total_dwelling_units','max_floor_lvl'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'units_per_floor', 'hdb_age_when_sold', 'tranc_year',\n",
       "       'mid_storey', 'floor_area_sqft', 'commercial', 'multistorey_carpark',\n",
       "       'precinct_pavilion', 'has_rental', 'mall_nearest_distance',\n",
       "       'mall_within_2km', 'hawker_nearest_distance', 'hawker_within_2km',\n",
       "       'hawker_food_stalls', 'hawker_market_stalls', 'mrt_nearest_distance',\n",
       "       'bus_interchange', 'mrt_interchange', 'bus_stop_nearest_distance',\n",
       "       'pri_sch_nearest_distance', 'vacancy', 'pri_sch_affiliation',\n",
       "       'sec_sch_nearest_dist', 'cutoff_point', 'affiliation',\n",
       "       'region_maturity_EAST MATURE', 'region_maturity_NORTH',\n",
       "       'region_maturity_NORTH-EAST', 'region_maturity_NORTH-EAST MATURE',\n",
       "       'region_maturity_WEST', 'region_maturity_WEST MATURE',\n",
       "       'flat_type_3 ROOM', 'flat_type_4 ROOM', 'flat_type_5 ROOM',\n",
       "       'flat_type_EXECUTIVE', 'model_category_Private Design',\n",
       "       'model_category_Standard', 'tranc_month_2', 'tranc_month_3',\n",
       "       'tranc_month_4', 'tranc_month_5', 'tranc_month_6', 'tranc_month_7',\n",
       "       'tranc_month_8', 'tranc_month_9', 'tranc_month_10', 'tranc_month_11',\n",
       "       'tranc_month_12'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dummifying categorical variables and one-hot encoding\n",
    "dummies_cols = ['region_maturity', 'flat_type', 'model_category','tranc_month']\n",
    "train_dummies = train.copy()\n",
    "for col in dummies_cols:\n",
    "    train_dummies = pd.get_dummies(train_dummies, columns=[col], prefix=col, drop_first=True)\n",
    "train_dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the cleaned data test set to the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a record of the post-processed list of ID before we do modelling\n",
    "# for us to merge the predicted resale prices to the respective IDs \n",
    "test_final_id = train_dummies[['id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define X matrix\n",
    "X = train_dummies.drop(columns='id') #STOP HERE, include scale on test data, after scaling, this is X, then predict for y\n",
    "\n",
    "#scale the values using StandardScaler()\n",
    "#this should only be done on continuous numerical columns as defined in the above list\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Transform the scaler on the training data for numerical columns\n",
    "numerical_cols = ['units_per_floor','hdb_age_when_sold','tranc_year','mid_storey','floor_area_sqft','mall_nearest_distance','mall_within_2km','hawker_nearest_distance','hawker_within_2km',\n",
    "                  'hawker_food_stalls','hawker_market_stalls','mrt_nearest_distance','bus_stop_nearest_distance','pri_sch_nearest_distance','vacancy','sec_sch_nearest_dist','cutoff_point']\n",
    "\n",
    "X[numerical_cols] = ss.fit_transform(X[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[339791.02082323 505581.93271814 363370.74330872 ... 411946.59958448\n",
      " 467060.41805439 377709.9161734 ]\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "ridge_cv = joblib.load(\"./trained.pkl\") \n",
    "\n",
    "# create y_preds\n",
    "# use the same variable name of the model saved in .pkl\n",
    "y_preds_ridge = ridge_cv.predict(X)\n",
    "print(y_preds_ridge)\n",
    "\n",
    "# round y_preds_ridge to the nearest thousand\n",
    "y_preds_ridge = np.round(y_preds_ridge,-3)\n",
    "\n",
    "# convert it to a pd series\n",
    "rounded_predicted_prices = pd.Series(y_preds_ridge, name='predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concat the predicted prices to the test_final_id\n",
    "# df_predicted = pd.concat([test_final_id, rounded_predicted_prices], axis=1)\n",
    "\n",
    "# # merge final result back to the original list of IDs\n",
    "# final_result = pd.merge(test_original_id, df_predicted, on='id', how='left')\n",
    "\n",
    "# # check for nan values and fill them with 0\n",
    "# final_result.isnull().sum()\n",
    "# final_result.fillna(0, inplace=True)\n",
    "\n",
    "# add the predicted values as a new column to the test_final_id\n",
    "test_final_id[\"Predicted\"] = y_preds_ridge\n",
    "# merge final result back to the original list of IDs\n",
    "final_result = pd.merge(test_original_id, test_final_id, on='id', how='left')\n",
    "# check for nan values and fill them with 0\n",
    "final_result.isnull().sum()\n",
    "final_result.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114982</td>\n",
       "      <td>340000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95653</td>\n",
       "      <td>506000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40303</td>\n",
       "      <td>363000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>109506</td>\n",
       "      <td>291000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100149</td>\n",
       "      <td>421000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  Predicted\n",
       "0  114982   340000.0\n",
       "1   95653   506000.0\n",
       "2   40303   363000.0\n",
       "3  109506   291000.0\n",
       "4  100149   421000.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a CSV file from a newly created dataframe containing predicted resale prices and IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save them to CSV file in the desired location\n",
    "final_result.to_csv('./data/prediction_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
